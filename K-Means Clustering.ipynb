{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering text documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:30.446247Z",
     "start_time": "2024-07-26T10:12:57.664766Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to wordnet...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.tokenize import word_tokenize #Used to extract words from documents\n",
    "import nltk\n",
    "nltk.download('punkt','wordnet')\n",
    "from nltk.stem import WordNetLemmatizer #Used to lemmatize words\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:42.442212Z",
     "start_time": "2024-07-26T10:13:42.427279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 newsgroups dataset for categories:\n",
      "['talk.politics.misc', 'sci.space', 'comp.graphics']\n"
     ]
    }
   ],
   "source": [
    "# Selected 3 categories from the datasets\n",
    "categories=['talk.politics.misc','sci.space','comp.graphics']\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:44.385950Z",
     "start_time": "2024-07-26T10:13:42.625560Z"
    }
   },
   "outputs": [],
   "source": [
    "df = fetch_20newsgroups(subset='all', categories=categories, \n",
    "                             shuffle=False, remove=('headers', 'footers', 'quotes'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:44.395374Z",
     "start_time": "2024-07-26T10:13:44.389112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "labels=df.target\n",
    "true_k=len(np.unique(labels))\n",
    "print(true_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T10:13:44.501506Z",
     "start_time": "2024-07-26T10:13:44.398366Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Lemmatizing the documents\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_data = []\n",
    "\n",
    "for i in range(len(df.data)):\n",
    "    word_list = word_tokenize(df.data[i])\n",
    "    lemmatized_doc = \" \".join([lemmatizer.lemmatize(word) for word in word_list])\n",
    "    lemmatized_data.append(lemmatized_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In regards to fractal commpression, I have seen 2 fractal compressed \"movies\".\n",
      "They were both fairly impressive.  The first one was a 64 gray scale \"movie\" of\n",
      "Casablanca, it was 1.3MB and had 11 minutes of 13 fps video.  It was a little\n",
      "grainy but not bad at all.  The second one I saw was only 3 minutes but it\n",
      "had 8 bit color with 10fps and measured in at 1.2MB.\n",
      "\n",
      "I consider the fractal movies a practical thing to explore.  But unlike many \n",
      "other formats out there, you do end up losing resolution.  I don't know what\n",
      "kind of software/hardware was used for creating the \"movies\" I saw but the guy\n",
      "that showed them to me said it took 5-15 minutes per frame to generate.  But as\n",
      "I said above playback was 10 or more frames per second.  And how else could you\n",
      "put 11 minutes on one floppy disk?\n"
     ]
    }
   ],
   "source": [
    "print(df.data[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next convert our corpus into tf-idf vectors. We remove common stop words, terms with very low document frequency (many of them are numbers or misspells), accents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer(strip_accents='unicode',stop_words='english',min_df=2)\n",
    "X=vectorizer.fit_transform(df.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2735, 16134)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering using standard K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first cluster documents using the standard k-means algorithm (actually, a refined variant called k-means++), without any further date preprocessing. The key parameter of choice when performing k-means is $k$. Alas, there really is no principled way to choose an initial value for $k$. Essentially we have two options:\n",
    "\n",
    "1. We choose a value that reflects our knowledge about the data, as in this case\n",
    "2. We may try several value, possibly in increasing order. We proceed this way as long as the quality of the resulting clustering (as measured by one or more quality indices) increases and stop when it starts decreasing. \n",
    "\n",
    "In this specific case, we set $k = 3$ of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 0.301s\n"
     ]
    }
   ],
   "source": [
    "km=KMeans(n_clusters=true_k,init='k-means++',max_iter=100)\n",
    "t0=time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard measures of cluster quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.372\n",
      "Completeness: 0.409\n",
      "V-measure: 0.389\n",
      "Adjusted Rand-Index: 0.326\n",
      "Silhouette Coefficient: 0.007\n"
     ]
    }
   ],
   "source": [
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify the 10 most relevant terms in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0: graphics thanks image file files know format program does looking\n",
      "Cluster 1: space like just think don shuttle know nasa orbit time\n",
      "Cluster 2: people don government think just men like right make did\n"
     ]
    }
   ],
   "source": [
    "centroids = km.cluster_centers_.argsort()[:, ::-1] ## Indices of largest centroids' entries in descending order\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualizationÂ¶\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_dict(cluster_index):\n",
    "    if cluster_index>true_k-1:\n",
    "        return cluster_index\n",
    "    term_frequency=km.cluster_centers_[cluster_index]\n",
    "    sorted_terms=centroids[cluster_index]\n",
    "    frequencies = {terms[i]: term_frequencies[i] for i in sorted_terms}\n",
    "    return frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
